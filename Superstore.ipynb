{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyanshuBITS/Machine-Learning-Project/blob/main/Superstore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr5mM5moOhMn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di0aXtt_On1m"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzdWANaROwmf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('superstore.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYSzgqmPnN7"
      },
      "source": [
        "**Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "focCrDjxPPVu"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxniVGfPlQ4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO9fpf8kRukd"
      },
      "source": [
        "There is no null value in dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCmx0UJiPmQx"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZgCx8TtPyVM"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3VVZES_TyKE"
      },
      "outputs": [],
      "source": [
        "df['Order Date'].unique().shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dBIEK2hWM5y"
      },
      "outputs": [],
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "orders_by_month_year = df.groupby([df['Order Date'].dt.year, df['Order Date'].dt.month]).size().unstack(fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(orders_by_month_year, cmap='viridis', annot=True, fmt='d')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Year')\n",
        "plt.title('Order Counts by Year and Month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6yOWA9JXrai"
      },
      "source": [
        "* The last four months of the year (September to December) are consistently the busiest period.\n",
        "* Consistent year over-year growth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4wuarMcXxjj"
      },
      "outputs": [],
      "source": [
        "#now let's see if there are missing values\n",
        "missing = df.isnull().sum()\n",
        "print(missing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC3aT9uTT29K"
      },
      "source": [
        "* There is no missing value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4mvRqQiT1o3"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The maximum sale is very high ($22,638),  compared to average $229, which strongly indicates the presence of outliers or a few very large orders.\n",
        "* The minimum profit is highly negative ($-6,599). This is showing that some sales are extremely unprofitable."
      ],
      "metadata": {
        "id": "DU-I8-NCcIZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5PgFANHUIMP"
      },
      "outputs": [],
      "source": [
        "#columns has unique values with their frequencies\n",
        "categorical_cols = ['Segment', 'Category', 'Sub-Category', 'Region', 'State', 'Ship Mode']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"\\n{col} :-\")\n",
        "        print(f\"Unique values: {df[col].nunique()}\")\n",
        "        print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Office Supplies is the most sold category by volume.\n",
        "* Binders and Paper are the most frequently sold individual product."
      ],
      "metadata": {
        "id": "3wKtFGFMdE6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9uwj-dzbtRG"
      },
      "outputs": [],
      "source": [
        "df['Ship Date']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ship date is not in datetime."
      ],
      "metadata": {
        "id": "CDCufLdTdbQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZxSftuNYdGs"
      },
      "outputs": [],
      "source": [
        "df['Order Date']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6r3R5-b_gl"
      },
      "source": [
        "* Since Order date is in datetime object there is no need for any conversion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network Analysis of Product Relationships**\n",
        "\n",
        "*Gonna see which products are frequently bought together."
      ],
      "metadata": {
        "id": "QQtgKKZZnrXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing data for network analysis\n",
        "#grouping by order id to get all product names in each order\n",
        "order_products = df.groupby('Order ID')['Product Name'].apply(list).reset_index()"
      ],
      "metadata": {
        "id": "3YP5F6BiDuJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering out orders with only one product as they don't form pairs\n",
        "order_products = order_products[order_products['Product Name'].apply(len) > 1]"
      ],
      "metadata": {
        "id": "5QmxLHH6EOCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "h-5i-4OmFItL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create edges (pairs of products) for each order using itertools.combinations\n",
        "edges = []\n",
        "for products_list in order_products['Product Name']:\n",
        "    # Sorting products in the list to ensure consistent pair order (for example-('A', 'B') is same as ('B', 'A'))\n",
        "    sorted_products = sorted(products_list)\n",
        "    for pair in combinations(sorted_products, 2):\n",
        "        edges.append(pair)"
      ],
      "metadata": {
        "id": "12wQJARuEQl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's count the frequency of each product pair\n",
        "edge_counts = pd.Series(edges).value_counts().reset_index()\n",
        "edge_counts.columns = ['Product Pair', 'Frequency']"
      ],
      "metadata": {
        "id": "hCY6DuJCGBTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the top most frequent product pairs for network visualization\n",
        "top_n_pairs = 25\n",
        "most_frequent_pairs = edge_counts.head(25)\n",
        "print(most_frequent_pairs)"
      ],
      "metadata": {
        "id": "PWriEJ26GLF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation - so there are 24 product pairs exists."
      ],
      "metadata": {
        "id": "Qu7SUfCiGg1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "h9W9O40lnq8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN3_LLEBY09q"
      },
      "outputs": [],
      "source": [
        "#building the network graph\n",
        "G = nx.Graph() #this will create an empty undirected graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding nodes and edges, here nodes are individual products and edges are co-purchase relationship\n",
        "for index, row in most_frequent_pairs.iterrows():\n",
        "    product1, product2 = row['Product Pair']\n",
        "    frequency = row['Frequency']\n",
        "    G.add_edge(product1, product2, weight = frequency)"
      ],
      "metadata": {
        "id": "Cdlrd7e_zGYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of the Network Graph"
      ],
      "metadata": {
        "id": "qtGKXcHZG0mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,16)) # I have increased figure size for better visualization\n",
        "pos = nx.spring_layout(G, k = 0.9, iterations = 70, seed = 42)\n",
        "# nodes (products)\n",
        "nx.draw_networkx_nodes(G, pos, node_size=4500, node_color='lightseagreen', alpha=0.9, linewidths=1.5, edgecolors='darkslategray')\n",
        "# edges(co-purchase links)\n",
        "max_freq = most_frequent_pairs['Frequency'].max()\n",
        "edge_widths = [d['weight'] / max_freq * 10 for u, v, d in G.edges(data=True)]\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', width=edge_widths, alpha=0.6)\n",
        "nx.draw_networkx_labels(G, pos, font_size=5, font_color='black')\n",
        "plt.title(f\"Network Analysis of Products Co-Purchases\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tjS1Nl6rGvcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* nodes that are very close together are highly connected to each other.\n",
        "In the spring_layout algorithm's simulation, the \"springs\" (representing edges/connections) between these nodes are very strong because the products they represent are very frequently co-purchased. These strong attractive forces pull them into tight clusters.These are natural product groupings or bundles that customers consistently buy together."
      ],
      "metadata": {
        "id": "g6RncG56X99e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This technique leverages graph theory to visually represent product co-purchase patterns. Instead of just showing lists of frequently bought items, it constructs a network where each 'node' is a product and a connection(edge) signifies co-purchase within an order. The thickness of the edge directly indicates the frequency and strength of this relationship."
      ],
      "metadata": {
        "id": "bqlje9fiZFYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Time-Series Clustering of Customer Behavior**"
      ],
      "metadata": {
        "id": "xFyDGEIXZr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here my goal is to identify distinct customer segments based on their monthly purchasing patterns over time.\n",
        "#now i know that order date is a datetime object\n",
        "df['Order Month'] = df['Order Date'].dt.to_period('M')\n",
        "#now i'm grouping the customer id and order month to get monthly sales for each customer\n",
        "customer_monthly_sales = df.groupby(['Customer ID', 'Order Month'])['Sales'].sum().unstack(fill_value=0)\n",
        "all_months = pd.period_range(start = df['Order Month'].min(), end = df['Order Month'].max(), freq = 'M') #ensuring all customers have time series of same length , M is for monthly frequency\n",
        "customer_monthly_sales = customer_monthly_sales.reindex(columns=all_months, fill_value=0)"
      ],
      "metadata": {
        "id": "zoyLt3vvHUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() #standard scaler ensures mean 0 and standard deviation 1.\n",
        "customer_monthly_sales_scaled = scaler.fit_transform(customer_monthly_sales)\n",
        "customer_monthly_sales_scaled = pd.DataFrame(customer_monthly_sales_scaled,index = customer_monthly_sales.index, columns=customer_monthly_sales.columns)"
      ],
      "metadata": {
        "id": "0IJnXCtPcR7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_monthly_sales_scaled.iloc[:5, :5])"
      ],
      "metadata": {
        "id": "d5KnwiquePbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* negative values are indicating that the customer's sales for a given month is less than the average sakes for that month across all customers."
      ],
      "metadata": {
        "id": "GqGeYE0NjmxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "metadata": {
        "id": "Rt20AZtRbfvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clustering\n",
        "n = 5\n",
        "cluster = AgglomerativeClustering(n_clusters=n)\n",
        "#customer_monthly_sales_scaled.columns = customer_monthly_sales_scaled.columns.astype(str)\n",
        "customer_monthly_sales['Cluster'] = cluster.fit_predict(customer_monthly_sales_scaled.values)"
      ],
      "metadata": {
        "id": "l0c1TfJpcCH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_monthly_sales['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "qfrqPaguc0z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_profiles = customer_monthly_sales.groupby('Cluster').mean()"
      ],
      "metadata": {
        "id": "9gXxRj4GiAvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_profiles.columns"
      ],
      "metadata": {
        "id": "LU61GscDMhHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_profiles.head()"
      ],
      "metadata": {
        "id": "-tJH7bb2Mpo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,10))\n",
        "#transposing the dataframe so that months are on the x axis and each column represent a cluster's average sales\n",
        "cluster_profiles_T = cluster_profiles.T"
      ],
      "metadata": {
        "id": "posybR8x2CYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot each cluster's average monthly sales over time\n",
        "plt.figure(figsize=(18, 10))\n",
        "for cluster_id in cluster_profiles_T.columns:\n",
        "    x_values = pd.to_datetime(cluster_profiles_T.index.astype(str))\n",
        "    y_values = cluster_profiles_T[cluster_id]\n",
        "    plt.plot(x_values, y_values, label=f'Cluster {cluster_id}')\n",
        "\n",
        "plt.title('Time Series Clustering of Customer Behavior', fontsize=20, weight='bold')\n",
        "plt.xlabel('Month', fontsize=14)\n",
        "plt.ylabel('Average Monthly Sales', fontsize=14)\n",
        "plt.legend(title='Customer Cluster')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-abjIm47YIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geospatial Analysis"
      ],
      "metadata": {
        "id": "LT9StBADIlKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* now creating a choropleth map of the United States to visualize sales and profitability by state. This is far more impactful than a simple bar chart, as it instantly reveals regional strengths, weaknesses, and potential market opportunities."
      ],
      "metadata": {
        "id": "X7YrEm4HGctC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "#let's prepare data by states\n",
        "df['State'] = df['State'].str.strip()#removing leading or trailing whitespaces from state names\n",
        "state_data = df.groupby('State').agg(Total_sales = ('Sales', 'sum'), Total_profit = ('Profit', 'sum'), Customer_Count = ('Customer ID', 'nunique')).reset_index()\n",
        "#profit  ratio\n",
        "state_data['Profit_Ratio'] = (state_data['Total_profit'] / state_data['Total_sales'])*100\n",
        "state_data.head()"
      ],
      "metadata": {
        "id": "12KkHLTt3d6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['State']"
      ],
      "metadata": {
        "id": "68C_hRnIgVWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #now creating the interactive map and coloring the states based on their total sales\n",
        "# fig = px.choropleth(state_data,\n",
        "#                     locations = 'State',\n",
        "#                     locationmode = 'USA-states',\n",
        "#                     color = 'Total_sales',\n",
        "#                     scope = 'usa',\n",
        "#                     hover_name='State',\n",
        "#                     hover_data={'Total_profit': ':.2f', 'Profit_Ratio': ':.2f', 'Customer_Count': True },\n",
        "#                     color_continuous_scale=\"Viridis\",\n",
        "#                     title=\"Geospatial Analysis: Total Sales & Profitability by State\" )\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "-MYhuyrxe3FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_data.info()"
      ],
      "metadata": {
        "id": "jiOn19J8h4JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recency, Frequency, and Monetary Calculation"
      ],
      "metadata": {
        "id": "sp3kkO4L15Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* recency is how recently a customer has made a purchase. Low recency value(higher recency score) shows recent purchase."
      ],
      "metadata": {
        "id": "gicjwC5w2h5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# snapshot date for recency calculation (one day after the last order)\n",
        "snapshot_date = df['Order Date'].max() + pd.Timedelta(days=1)\n",
        "rfm_data = df.groupby('Customer ID').agg({'Order Date': lambda date: (snapshot_date - date.max()).days,'Order ID': 'nunique', 'Sales': 'sum'})\n",
        "rfm_data.rename(columns={'Order Date': 'Recency','Order ID': 'Frequency','Sales': 'MonetaryValue'}, inplace=True)\n",
        "#Lower Recency is better, Higher Frequency and Monetary are better.\n",
        "r_labels = range(5, 0, -1)\n",
        "f_labels = range(1, 6)\n",
        "m_labels = range(1, 6)\n",
        "rfm_data['R_Score'] = pd.qcut(rfm_data['Recency'], 5, labels=r_labels, duplicates='drop').astype(int)\n",
        "rfm_data['F_Score'] = pd.qcut(rfm_data['Frequency'], 5, labels=f_labels, duplicates='drop').astype(int)\n",
        "rfm_data['M_Score'] = pd.qcut(rfm_data['MonetaryValue'], 5, labels=m_labels, duplicates='drop').astype(int)\n",
        "#using the weighted formula: 50% Recency, 25% Frequency, 25% Monetary\n",
        "rfm_data['Customer_Health_Score'] = (0.5 * rfm_data['R_Score']) + (0.25 * rfm_data['F_Score']) + (0.25 * rfm_data['M_Score'])\n",
        "# Normalize the score to a 0-100\n",
        "max_possible_score = (0.5*5) + (0.25*5) + (0.25*5) # Max score is 5\n",
        "min_possible_score = (0.5*1) + (0.25*1) + (0.25*1) # Min score is 1\n",
        "rfm_data['Customer_Health_Score'] = ((rfm_data['Customer_Health_Score'] - min_possible_score) / (max_possible_score - min_possible_score)) * 100\n",
        "#Display the top customers by health score\n",
        "print(rfm_data.sort_values('Customer_Health_Score', ascending=False).head())"
      ],
      "metadata": {
        "id": "FNrKafD5ivUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate data by product\n",
        "product_data = df.groupby('Product Name').agg(Total_Sales=('Sales', 'sum'),Total_Profit=('Profit', 'sum')).reset_index()\n",
        "# Filter out products with zero or negative sales to avoid division by zero\n",
        "product_data = product_data[product_data['Total_Sales'] > 0]\n",
        "product_data['Profit_Margin'] = (product_data['Total_Profit'] / product_data['Total_Sales']) * 100\n",
        "# Rank products by sales and profit margin\n",
        "product_data['Sales_Rank_Score'] = product_data['Total_Sales'].rank(pct=True)\n",
        "product_data['Margin_Rank_Score'] = product_data['Profit_Margin'].rank(pct=True)\n",
        "# We give equal weight to sales volume and profitability\n",
        "product_data['Product_Velocity_Index'] = (product_data['Sales_Rank_Score'] + product_data['Margin_Rank_Score']) * 50 # Scale to 100\n",
        "# Display the top products by velocity index\n",
        "print(product_data.sort_values('Product_Velocity_Index', ascending=False).head())"
      ],
      "metadata": {
        "id": "TRQkXBVUOqf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* now calculating the number of unique customers in each state to identify regions with a strong customer base."
      ],
      "metadata": {
        "id": "g0HZihYCm7tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate unique customers per state\n",
        "market_penetration = df.groupby('State')['Customer ID'].nunique().reset_index()\n",
        "market_penetration.rename(columns={'Customer ID': 'Unique_Customer_Count'}, inplace=True)\n",
        "market_penetration = market_penetration.sort_values('Unique_Customer_Count', ascending=False)\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(x='Unique_Customer_Count', y='State', data=market_penetration, palette='coolwarm')\n",
        "plt.title('Market Penetration: Unique Customers per State', fontsize=16)\n",
        "plt.xlabel('Number of Unique Customers', fontsize=12)\n",
        "plt.ylabel('State', fontsize=12)\n",
        "plt.show()\n",
        "print(\"--- Top 5 States by Customer Count ---\")\n",
        "print(market_penetration.head())\n",
        "print(\"\\n--- Bottom 5 States by Customer Count ---\")\n",
        "print(market_penetration.tail())"
      ],
      "metadata": {
        "id": "E38e_836XifN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Ship Date'] = pd.to_datetime(df['Ship Date'])\n",
        "# Defining promised shipping times\n",
        "promised_times = {'Same Day': 0,'First Class': 1,'Second Class': 3,'Standard Class': 5}\n",
        "df['Promised_Time'] = df['Ship Mode'].map(promised_times)\n",
        "# Calculating actual shipping time\n",
        "df['Actual_Ship_Time'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
        "# checking if each order was on time -\n",
        "df['Is_On_Time'] = df['Actual_Ship_Time'] <= df['Promised_Time']\n",
        "\n",
        "on_time_percentage = df['Is_On_Time'].mean() * 100\n",
        "print(f\"--- Operational Excellence Score ---\")\n",
        "print(f\"On-Time Shipping Percentage: {on_time_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "oAF1dkdmXxmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate data by Category\n",
        "category_data = df.groupby('Category').agg( Total_Sales=('Sales', 'sum'), Total_Profit=('Profit', 'sum')).reset_index()\n",
        "\n",
        "# Calculate Market Share and Profit Margin\n",
        "total_company_sales = category_data['Total_Sales'].sum()\n",
        "category_data['Market_Share'] = (category_data['Total_Sales'] / total_company_sales)\n",
        "category_data['Profit_Margin'] = (category_data['Total_Profit'] / category_data['Total_Sales'])\n",
        "category_data['Market_Share_Score'] = category_data['Market_Share'].rank(pct=True)\n",
        "category_data['Profit_Margin_Score'] = category_data['Profit_Margin'].rank(pct=True)\n",
        "category_data['Competitive_Advantage_Index'] = (category_data['Market_Share_Score'] + category_data['Profit_Margin_Score']) * 50\n",
        "print(category_data.sort_values('Competitive_Advantage_Index', ascending=False))"
      ],
      "metadata": {
        "id": "GZMBt24eeeM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Survival Analysis For Customer Churn**"
      ],
      "metadata": {
        "id": "beerD5Sii3tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lifelines"
      ],
      "metadata": {
        "id": "k7PxvTsqlYS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lifelines import CoxPHFitter"
      ],
      "metadata": {
        "id": "q6Fuk_J5TNtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering\n",
        "customer_features = df.groupby('Customer ID').agg(\n",
        "    duration=('Order Date', lambda date: (date.max() - date.min()).days),\n",
        "    Total_Sales=('Sales', 'sum'),\n",
        "    Avg_Discount=('Discount', 'mean'),\n",
        "    Total_Orders=('Order ID', 'nunique')\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "z1HkhDptlLFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's define churn event by a common business rule: a customer is considered \"churned\" if their last purchase was more than 6 months ago.\n",
        "observation_end_date = df['Order Date'].max()\n",
        "churn_cutoff_date = observation_end_date - pd.DateOffset(months=6)\n",
        "# the last purchase date for each customer\n",
        "last_purchase_dates = df.groupby('Customer ID')['Order Date'].max().reset_index()\n",
        "last_purchase_dates.rename(columns={'Order Date': 'Last_Purchase_Date'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, last_purchase_dates, on='Customer ID')\n",
        "# The 'churned' event is 1 if their last purchase was before the cutoff, else 0.\n",
        "customer_features['churned'] = (customer_features['Last_Purchase_Date'] < churn_cutoff_date).astype(int)"
      ],
      "metadata": {
        "id": "YqCIpO0WmUTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cox Proportional Hazards Model - This model will tell us the effect of each feature on the likelihood of churn.\n",
        "cph = CoxPHFitter()\n",
        "cph.fit(customer_features[['duration', 'churned', 'Total_Sales', 'Avg_Discount', 'Total_Orders']], duration_col='duration', event_col='churned')"
      ],
      "metadata": {
        "id": "oSmVkyTFnsNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cph.print_summary()"
      ],
      "metadata": {
        "id": "IjtgEjVYorgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concept -\n",
        "exp(coef)-(Hazard Ratio):\n",
        "1. If this is < 1, the factor reduces the risk of churn (it's a good thing).\n",
        "2. If this is > 1, the factor increases the risk of churn (it's a bad thing).\n",
        "\n",
        "p (p-value):\n",
        "\n",
        "1. If this is < 0.05, the factor's effect is statistically significant and real.\n",
        "\n",
        "2. If this is > 0.05, the effect is likely due to random chance and not significant."
      ],
      "metadata": {
        "id": "7d95Ea-RvIIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total_Orders: The exp(coef) is 0.63 (which is less than 1) and the p-value is <0.005.\n",
        "Insight: This is our most important finding. It means that for every additional order a customer places, their risk of churning decreases by about 37% (1.00 - 0.63 = 0.37). This is a highly significant factor that drives customer retention.\n",
        "\n",
        "Avg_Discount & Total_Sales: For both of these factors, the p-value is very high (0.92 and 0.89).\n",
        "Insight: This tells us that, surprisingly, the total amount a customer spends and the average discount they receive have no statistically significant effect on whether they churn or not."
      ],
      "metadata": {
        "id": "EQdmDEJqvvc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cph.plot()"
      ],
      "metadata": {
        "id": "0cfwpJG7oy1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The vertical dashed line represents \"no effect.\"\n",
        "* Factors to the left of the line reduce churn risk.\n",
        "* Factors to the right increase churn risk."
      ],
      "metadata": {
        "id": "kidUKSZtwoS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian A/B Testing for Discount Effectiveness**"
      ],
      "metadata": {
        "id": "qNZ7GEsZx0UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's answer the question - \"How likely is it that offering a discount is better than offering no discount?\""
      ],
      "metadata": {
        "id": "BYqLtcgIx4ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import arviz as az"
      ],
      "metadata": {
        "id": "Z9ISl0QEo2Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[df['Quantity'] < 10].copy() #filtering data for simplicity"
      ],
      "metadata": {
        "id": "VtzeD0atyGa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group A (Control)\n",
        "control_group = data[data['Discount'] == 0]['Quantity']\n",
        "# Group B (Treatment)\n",
        "treatment_group = data[data['Discount'] > 0]['Quantity']"
      ],
      "metadata": {
        "id": "gX0XTTFSyXm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(control_group))#no discount\n",
        "print(len(treatment_group))#with discount"
      ],
      "metadata": {
        "id": "mSWycemByha_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's define bayesian model\n",
        "with pm.Model() as model:\n",
        "    avg_control = pm.Gamma('avg_control', alpha=1.0, beta=1.0)\n",
        "    avg_treatment = pm.Gamma('avg_treatment', alpha=1.0, beta=1.0)\n",
        "    control_likelihood = pm.Poisson('control_likelihood', mu=avg_control, observed=control_group)\n",
        "    treatment_likelihood = pm.Poisson('treatment_likelihood', mu=avg_treatment, observed=treatment_group)\n",
        "    diff_of_means = pm.Deterministic('diff_of_means', avg_treatment - avg_control)\n",
        "    treatment_is_better = pm.Deterministic('treatment_is_better', diff_of_means > 0)\n",
        "#running the simulation\n",
        "with model:\n",
        "    trace = pm.sample(2000, tune=1000, cores=1)\n"
      ],
      "metadata": {
        "id": "y5FJWBdbyobp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's analyse and plot\n",
        "az.plot_posterior(trace, var_names=['avg_control', 'avg_treatment', 'diff_of_means'])\n",
        "\n"
      ],
      "metadata": {
        "id": "HbqJJu4zzlGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This analysis strongly indicates that offering a discount is not an effective strategy for increasing the number of items customers purchase per transaction."
      ],
      "metadata": {
        "id": "jnW8GCJi0pg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Market Share Velocity Calculation"
      ],
      "metadata": {
        "id": "wNsfXGcJ02La"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to measure the rate of change of market share for our main product categories. A simple market share number tells you where you are now, but velocity tells you if you are gaining or losing ground and how quickly. This is a leading indicator of future performance."
      ],
      "metadata": {
        "id": "J-E3L5AWCQ7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "df['YearQuarter'] = df['Order Date'].dt.to_period('Q').astype(str)\n",
        "# total sales per quarter\n",
        "total_quarterly_sales = df.groupby('YearQuarter')['Sales'].sum().reset_index()\n",
        "total_quarterly_sales.rename(columns={'Sales': 'Total_Quarterly_Sales'}, inplace=True)\n",
        "# sales per category per quarter\n",
        "category_quarterly_sales = df.groupby(['YearQuarter', 'Category'])['Sales'].sum().reset_index()\n",
        "#calculate market share\n",
        "market_data = pd.merge(category_quarterly_sales, total_quarterly_sales, on='YearQuarter')\n",
        "market_data['Market_Share'] = (market_data['Sales'] / market_data['Total_Quarterly_Sales']) * 100\n",
        "# Calculate Market Share Velocity\n",
        "market_data.sort_values(by=['Category', 'YearQuarter'], inplace=True)\n",
        "# Calculate the change from the previous quarter\n",
        "market_data['Market_Share_Velocity'] = market_data.groupby('Category')['Market_Share'].pct_change() * 100\n",
        "# Show the velocity for the last few quarters for each category\n",
        "print(\"--- Market Share Velocity (% Change from Previous Quarter) ---\")\n",
        "print(market_data.dropna().groupby('Category').tail(3))\n",
        "avg_velocity = market_data.groupby('Category')['Market_Share_Velocity'].mean().reset_index()\n",
        "print(\"\\n--- Average Quarterly Market Share Velocity ---\")\n",
        "print(avg_velocity)"
      ],
      "metadata": {
        "id": "pMUK52NKCLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first thing we notice is how much the velocity fluctuates. For example, Furniture's market share grew by a massive 74.7% in Q2 but then fell by 14.8% in Q3. This indicates that market share is not stable and is highly dependent on sales within a specific quarter.\n",
        "*  In the most recent quarter (2017Q4), both Furniture (+12.5%) and Technology (+8.3%) saw healthy growth in their market share, while Office Supplies (-17.5%) saw a significant decline.\n",
        "* With an average quarterly growth of 5.7%, the Office Supplies category is consistently expanding its share of the business over the long term.The Furniture category is also growing steadily, with an average quarterly velocity of 3.8%.The Technology category has an average velocity of only 0.2%. This is a critical insight. While we know from previous analysis that Technology is the most profitable and has the highest market share, this result tells us that its dominant position is stable but not growing."
      ],
      "metadata": {
        "id": "X1iGkUPxVXM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Product Portfolio Health Score**"
      ],
      "metadata": {
        "id": "1ni-prqdZr3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate data by product\n",
        "product_health_data = df.groupby('Product Name').agg(Total_Sales=('Sales', 'sum'), Total_Profit=('Profit', 'sum'), Avg_Discount=('Discount', 'mean')).reset_index()\n",
        "# Filter out products with zero or negative sales to avoid division by zero\n",
        "product_health_data = product_health_data[product_health_data['Total_Sales'] > 0]\n",
        "product_health_data['Profit_Margin'] = product_health_data['Total_Profit'] / product_health_data['Total_Sales']\n",
        "product_health_data['Sales_Score'] = product_health_data['Total_Sales'].rank(pct=True) # Sales Score: Higher sales = higher score\n",
        "# Profit Score: Higher profit margin = higher score\n",
        "product_health_data['Profit_Score'] = product_health_data['Profit_Margin'].rank(pct=True)\n",
        "# Discount Score: Lower discount = higher score\n",
        "product_health_data['Discount_Score'] = 1 - product_health_data['Avg_Discount'].rank(pct=True)\n",
        "\n",
        "# Final Health Score\n",
        "# We give equal weight to each of the three components\n",
        "product_health_data['Health_Score'] = (product_health_data['Sales_Score'] + product_health_data['Profit_Score'] + product_health_data['Discount_Score']) / 3 * 100 # Scale to 100\n",
        "\n",
        "print(\"--- Top 10 Healthiest Products ---\")\n",
        "print(product_health_data.sort_values('Health_Score', ascending=False).head(10)[['Product Name', 'Health_Score', 'Total_Sales', 'Profit_Margin', 'Avg_Discount']])\n",
        "\n",
        "print(\"\\n--- Bottom 10 Unhealthiest Products ---\")\n",
        "print(product_health_data.sort_values('Health_Score', ascending=True).head(10)[['Product Name', 'Health_Score', 'Total_Sales', 'Profit_Margin', 'Avg_Discount']])\n"
      ],
      "metadata": {
        "id": "ZZJH4TNmC1Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here health score was evaluated by giving equal weightage to sales volume, profit margin, and discount level."
      ],
      "metadata": {
        "id": "b4eJLSDQbEOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Revenue Quality Score**"
      ],
      "metadata": {
        "id": "pX0DN1ivbO8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal is assess the health and sustainability of the revenue generated by each product category."
      ],
      "metadata": {
        "id": "ElGDx5QkdHpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A sale is 'High Quality' if it was profitable AND had no discount.\n",
        "df['is_high_quality'] = (df['Profit'] > 0) & (df['Discount'] == 0)\n",
        "df['high_quality_revenue'] = df.apply(lambda row: row['Sales'] if row['is_high_quality'] else 0, axis=1)\n",
        "# Calculate total sales for each category\n",
        "total_sales_by_category = df.groupby('Category')['Sales'].sum()\n",
        "# Calculate the high-quality revenue for each category\n",
        "high_quality_sales_by_category = df.groupby('Category')['high_quality_revenue'].sum()\n",
        "revenue_quality_df = pd.DataFrame({'Total_Revenue': total_sales_by_category, 'High_Quality_Revenue': high_quality_sales_by_category})\n",
        "revenue_quality_df['Revenue_Quality_Score'] = (revenue_quality_df['High_Quality_Revenue'] / revenue_quality_df['Total_Revenue']) * 100\n",
        "\n",
        "# Display the results ---\n",
        "print(\"--- Revenue Quality Score by Category ---\")\n",
        "print(revenue_quality_df.sort_values('Revenue_Quality_Score', ascending=False))\n"
      ],
      "metadata": {
        "id": "-8F7ZldlbOdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "result shows Office Supplies is the most sustainable category. The biggest strategic challenge is the Furniture category."
      ],
      "metadata": {
        "id": "KpLnHIgRdlG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical Hypothesis Generation**"
      ],
      "metadata": {
        "id": "i8hUzbMseo5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Test 1: Technology vs. Furniture Profit"
      ],
      "metadata": {
        "id": "BWSrkvGxe0hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind"
      ],
      "metadata": {
        "id": "mgba8NE-e_Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two separate samples for the test\n",
        "# Sample 1: Profit from all sales in the 'Technology' category\n",
        "tech_profit = df[df['Category'] == 'Technology']['Profit']\n",
        "# Sample 2: Profit from all sales in the 'Furniture' category\n",
        "furniture_profit = df[df['Category'] == 'Furniture']['Profit']\n",
        "# Perform the Independent Samples T-Test\n",
        "t_statistic, p_value = ttest_ind(tech_profit, furniture_profit, equal_var=False) # equal_var=False because the variances of the two groups might not be equal\n",
        "\n",
        "print(\"--- Hypothesis Test: Technology Profit vs. Furniture Profit ---\")\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05  # Standard significance level\n",
        "if p_value < alpha:\n",
        "    print(\"We reject the null hypothesis. The average profit for Technology is significantly different from Furniture.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis. There is no statistically significant difference in average profit.\")\n"
      ],
      "metadata": {
        "id": "otNFO2cAc3lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-statistic: 6.7320: This indicates that the average profit for Technology is indeed much higher than the average profit for Furniture.\n",
        "P-value: 0.0000: p-value this low means that the difference we observed is not due to random chance."
      ],
      "metadata": {
        "id": "pmx7j_-UgDWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Test 2: First Class vs. Standard Class Sales"
      ],
      "metadata": {
        "id": "oSd2Qaq7hF14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1: Sales from all orders with 'First Class' shipping\n",
        "first_class_sales = df[df['Ship Mode'] == 'First Class']['Sales']\n",
        "# Sample 2: Sales from all orders with 'Standard Class' shipping\n",
        "standard_class_sales = df[df['Ship Mode'] == 'Standard Class']['Sales']\n",
        "t_statistic, p_value = ttest_ind(first_class_sales, standard_class_sales, equal_var=False)\n",
        "\n",
        "print(\"--- Hypothesis Test: First Class Sales vs. Standard Class Sales ---\")\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion based on the p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"We reject the null hypothesis. The average sales value for First Class shipping is significantly different from Standard Class.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis. There is no statistically significant difference in average sales value.\")"
      ],
      "metadata": {
        "id": "kldyVtl8gXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "observation - the customers who choose premium 'First Class' shipping do not spend more on average than customers who choose 'Standard Class' shipping."
      ],
      "metadata": {
        "id": "Lh0BTt4Zhsl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shipping Mode and Sales Value"
      ],
      "metadata": {
        "id": "HHohMfr3kLKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Orders shipped via 'First Class' have a significantly higher average sales value than orders shipped via 'Standard Class'.\""
      ],
      "metadata": {
        "id": "M9y7O0fPkQM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1: Sales from all orders with 'First Class' shipping\n",
        "first_class_sales = df[df['Ship Mode'] == 'First Class']['Sales']\n",
        "\n",
        "# Sample 2: Sales from all orders with 'Standard Class' shipping\n",
        "standard_class_sales = df[df['Ship Mode'] == 'Standard Class']['Sales']\n",
        "\n",
        "# Perform the Independent Samples T-Test\n",
        "t_statistic, p_value = ttest_ind(first_class_sales, standard_class_sales, equal_var=False)\n",
        "print(\"--- Hypothesis Test: First Class Sales vs. Standard Class Sales ---\")\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"We reject the null hypothesis. The average sales value for First Class shipping is significantly different from Standard Class.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis. There is no statistically significant difference in average sales value.\")"
      ],
      "metadata": {
        "id": "4_oFuzfbh0MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Sales and Profit Dashboard"
      ],
      "metadata": {
        "id": "4N5aNL2gku-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dash"
      ],
      "metadata": {
        "id": "vpbjc9H_l2AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "\n",
        "\n",
        "# Initialize the Dash App\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# now defining the App Layout\n",
        "app.layout = html.Div(children=[\n",
        "    html.H1(children='Superstore Analytics Dashboard', style={'textAlign': 'center'}),\n",
        "\n",
        "    html.Div(children='''\n",
        "        An interactive dashboard to explore Sales and Profit data.\n",
        "    ''', style={'textAlign': 'center'}),\n",
        "\n",
        "    # Dropdown menu to select the metric\n",
        "    dcc.Dropdown(\n",
        "        id='metric-selector',\n",
        "        options=[\n",
        "            {'label': 'Sales', 'value': 'Sales'},\n",
        "            {'label': 'Profit', 'value': 'Profit'}\n",
        "        ],\n",
        "        value='Sales',  # Default value\n",
        "        style={'width': '50%', 'margin': '20px auto'}\n",
        "    ),\n",
        "\n",
        "    # Graph components that will be updated by the callback\n",
        "    dcc.Graph(id='category-bar-chart'),\n",
        "    dcc.Graph(id='time-series-chart')\n",
        "])\n",
        "\n",
        "# Define the Callback Function(This function connects the dropdown to the graphs)\n",
        "@app.callback(\n",
        "    [Output('category-bar-chart', 'figure'),\n",
        "     Output('time-series-chart', 'figure')],\n",
        "    [Input('metric-selector', 'value')]\n",
        ")\n",
        "def update_graphs(selected_metric):\n",
        "    # Bar Chart by Category\n",
        "    bar_fig = px.bar(\n",
        "        df.groupby('Category')[selected_metric].sum().reset_index(),\n",
        "        x='Category',\n",
        "        y=selected_metric,\n",
        "        title=f'Total {selected_metric} by Product Category'\n",
        "    )\n",
        "\n",
        "    # Time Series Chart\n",
        "    time_series_data = df.groupby(pd.Grouper(key='Order Date', freq='M'))[selected_metric].sum().reset_index()\n",
        "    line_fig = px.line(\n",
        "        time_series_data,\n",
        "        x='Order Date',\n",
        "        y=selected_metric,\n",
        "        title=f'Total {selected_metric} Over Time'\n",
        "    )\n",
        "\n",
        "    return bar_fig, line_fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, port=8051)"
      ],
      "metadata": {
        "id": "IoTFWX9DuUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "# Create a 'Month-Year' column for grouping\n",
        "df['MonthYear'] = df['Order Date'].dt.to_period('M').astype(str)\n",
        "# Group by month and category to get monthly sales\n",
        "monthly_sales = df.groupby(['MonthYear', 'Category'])['Sales'].sum().reset_index()\n",
        "# Sort by date to ensure correct cumulative calculation\n",
        "monthly_sales.sort_values(by='MonthYear', inplace=True)\n",
        "# Calculate the cumulative sales for each category\n",
        "monthly_sales['Cumulative_Sales'] = monthly_sales.groupby('Category')['Sales'].cumsum()\n",
        "fig = px.bar(\n",
        "    monthly_sales,\n",
        "    x='Category',\n",
        "    y='Cumulative_Sales',\n",
        "    color='Category',\n",
        "    animation_frame='MonthYear', # Creates a frame for each month\n",
        "    animation_group='Category',  # Tracks each category across frames\n",
        "    range_y=[0, monthly_sales['Cumulative_Sales'].max() * 1.1],\n",
        "    title=\"Animated Bar Chart Race: Cumulative Sales by Category\",\n",
        "    labels={'Cumulative_Sales': 'Cumulative Sales', 'Category': 'Product Category'}\n",
        ")\n",
        "# Improve the layout and animation speed\n",
        "fig.update_layout(\n",
        "    transition={'duration': 200} # Speed of transition between frames in milliseconds\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "cj4G6MPCqimk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inteteractive product co-purchase network"
      ],
      "metadata": {
        "id": "9KV-7oybM0va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "most_frequent_pairs = edge_counts.head(30)\n",
        "G = nx.from_pandas_edgelist(most_frequent_pairs,\n",
        "                             source='Product Pair',\n",
        "                             target='Product Pair', # This is a trick to handle tuples in columns\n",
        "                             edge_attr='Frequency')\n",
        "G_corrected = nx.Graph()\n",
        "for index, row in most_frequent_pairs.iterrows():\n",
        "    product1, product2 = row['Product Pair']\n",
        "    frequency = row['Frequency']\n",
        "    G_corrected.add_edge(product1, product2, weight=frequency)\n",
        "\n",
        "G = G_corrected\n",
        "# Calculate node positions using a spring layout\n",
        "pos = nx.spring_layout(G, k=0.8, iterations=50, seed=42)\n",
        "# Create the edge trace\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "for edge in G.edges():\n",
        "    x0, y0 = pos[edge[0]]\n",
        "    x1, y1 = pos[edge[1]]\n",
        "    edge_x.extend([x0, x1, None])\n",
        "    edge_y.extend([y0, y1, None])\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "# Create the node trace\n",
        "node_x = []\n",
        "node_y = []\n",
        "for node in G.nodes():\n",
        "    x, y = pos[node]\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='YlGnBu',\n",
        "        reversescale=True,\n",
        "        color=[],\n",
        "        size=10,\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Connections',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line_width=2))\n",
        "# Add node text (product names) and color based on connectivity\n",
        "node_adjacencies = []\n",
        "node_text = []\n",
        "for node, adjacencies in enumerate(G.adjacency()):\n",
        "    node_adjacencies.append(len(adjacencies[1]))\n",
        "    node_text.append(f'{adjacencies[0]}<br># of connections: {len(adjacencies[1])}')\n",
        "\n",
        "node_trace.marker.color = node_adjacencies\n",
        "node_trace.text = node_text\n",
        "\n",
        "# Create the Figure and Show It\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "             layout=go.Layout(\n",
        "                title='<br>Interactive Network Graph of Product Co-Purchases',\n",
        "                titlefont_size=16,\n",
        "                showlegend=False,\n",
        "                hovermode='closest',\n",
        "                margin=dict(b=20,l=5,r=5,t=40),\n",
        "                annotations=[ dict(\n",
        "                    text=\"An interactive graph showing which products are most frequently bought together.\",\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\", yref=\"paper\",\n",
        "                    x=0.005, y=-0.002 ) ],\n",
        "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                )\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "s3kR57IYM-j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEO Executive Dashboard"
      ],
      "metadata": {
        "id": "yfHmZaXnOCTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "df.sort_values('Order Date', inplace=True)\n",
        "monthly_summary = df.resample('M', on='Order Date').agg(\n",
        "    Monthly_Sales=('Sales', 'sum'),\n",
        "    Monthly_Profit=('Profit', 'sum')\n",
        ").reset_index()\n",
        "# Calculate cumulative unique customers over time\n",
        "df['cumulative_customers'] = df['Customer ID'].cumsum().astype(str).rank(method='dense').astype(int)\n",
        "monthly_customers = df.groupby(pd.Grouper(key='Order Date', freq='M'))['cumulative_customers'].max().reset_index()\n",
        "# using subplots to arrange the three charts\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    specs=[[{\"colspan\": 2}, None],\n",
        "           [{}, {}]],\n",
        "    subplot_titles=(\"Total Sales Over Time\", \"Total Profit Over Time\", \"Cumulative Customer Growth\")\n",
        ")\n",
        "#Chart 1: Total Sales Over Time\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_summary['Order Date'], y=monthly_summary['Monthly_Sales'], mode='lines+markers', name='Sales'),\n",
        "    row=1, col=1\n",
        ")\n",
        "# Chart 2: Total Profit Over Time\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_summary['Order Date'], y=monthly_summary['Monthly_Profit'], mode='lines+markers', name='Profit'),\n",
        "    row=2, col=1\n",
        ")\n",
        "# Chart 3: Cumulative Customer Growth\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_customers['Order Date'], y=monthly_customers['cumulative_customers'], mode='lines', name='Customers', line=dict(color='green')),\n",
        "    row=2, col=2\n",
        ")\n",
        "#Style and Finalize the Dashboard\n",
        "fig.update_layout(\n",
        "    title_text='Key Business Performance Indicators',\n",
        "    height=600,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "gCcPcszjNeJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "# Create a monthly summary for time-series analysis\n",
        "monthly_financials = df.resample('M', on='Order Date').agg(\n",
        "    Monthly_Sales=('Sales', 'sum'),\n",
        "    Monthly_Profit=('Profit', 'sum')\n",
        ").reset_index()\n",
        "# Calculate monthly profit margin\n",
        "monthly_financials['Profit_Margin'] = (monthly_financials['Monthly_Profit'] / monthly_financials['Monthly_Sales']) * 100\n",
        "# Create a summary by sub-category\n",
        "subcategory_profit = df.groupby('Sub-Category')['Profit'].sum().reset_index().sort_values('Profit', ascending=False)\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    specs=[[{\"colspan\": 2}, None],\n",
        "           [{}, {}]],\n",
        "    subplot_titles=(\"Profit Margin Over Time (%)\", \"Sales vs. Profit Analysis\", \"Profit by Sub-Category\")\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_financials['Order Date'], y=monthly_financials['Profit_Margin'], mode='lines', name='Profit Margin'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Sales vs. Profit\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_financials['Order Date'], y=monthly_financials['Monthly_Sales'], name='Sales', line=dict(color='blue')),\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_financials['Order Date'], y=monthly_financials['Monthly_Profit'], name='Profit', line=dict(color='red')),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Profit by Sub-Category\n",
        "fig.add_trace(\n",
        "    go.Bar(x=subcategory_profit['Sub-Category'], y=subcategory_profit['Profit'], name='Sub-Category Profit'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "\n",
        "#  Style and Finalize the Dashboard\n",
        "fig.update_layout(\n",
        "    title_text='Financial Performance & Profitability Analysis',\n",
        "    height=700,\n",
        "    showlegend=True\n",
        ")\n",
        "# Make the sub-category labels easier to read\n",
        "fig.update_xaxes(tickangle=45, row=2, col=2)\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "lB1Zv_6POIQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "df['MonthYear'] = df['Order Date'].dt.to_period('M')\n",
        "# Find the most recent month in the dataset\n",
        "last_month = df['MonthYear'].max()\n",
        "previous_month = last_month - 1\n",
        "# Filter data for the last month\n",
        "last_month_data = df[df['MonthYear'] == last_month]\n",
        "# Filter data for the previous month\n",
        "previous_month_data = df[df['MonthYear'] == previous_month]\n",
        "\n",
        "last_month_sales = last_month_data['Sales'].sum()\n",
        "previous_month_sales = previous_month_data['Sales'].sum()\n",
        "last_month_profit = last_month_data['Profit'].sum()\n",
        "previous_month_profit = previous_month_data['Profit'].sum()\n",
        "\n",
        "# Calculate sales growth\n",
        "sales_growth = ((last_month_sales - previous_month_sales) / previous_month_sales) * 100\n",
        "\n",
        "# Best and Worst Sub-Category in the Last Month\n",
        "subcat_performance = last_month_data.groupby('Sub-Category')['Profit'].sum()\n",
        "best_subcat = subcat_performance.idxmax()\n",
        "best_subcat_profit = subcat_performance.max()\n",
        "\n",
        "worst_subcat = subcat_performance.idxmin()\n",
        "worst_subcat_profit = subcat_performance.min()\n",
        "\n",
        "# Generate the Natural Language Summary\n",
        "report = f\"\"\"\n",
        "Report for Month: {last_month}\n",
        "1.  Overall Performance:\n",
        "    * Total Sales: ${last_month_sales:,.2f}\n",
        "    * Total Profit: ${last_month_profit:,.2f}\n",
        "    * Sales Growth (vs. prior month): {sales_growth:.2f}%\n",
        "\n",
        "2.  Key Insights for the Month:\n",
        "    * The best-performing product line was **'{best_subcat}'**, which generated **${best_subcat_profit:,.2f}** in profit.\n",
        "    * The worst-performing product line was **'{worst_subcat}'**, which lost **${worst_subcat_profit:,.2f}**.\n",
        "\"\"\"\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "-UFBX1a_PGF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import plotly.graph_objects as go\n",
        "# Prophet requires the columns to be named 'ds' (datestamp) and 'y' (value).\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "monthly_sales = df.resample('M', on='Order Date')['Sales'].sum().reset_index()\n",
        "monthly_sales.rename(columns={'Order Date': 'ds', 'Sales': 'y'}, inplace=True)\n",
        "# We will train the model on data up to the end of 2016 and test its accuracy on 2017 data.\n",
        "train = monthly_sales[monthly_sales['ds'] < '2017-01-01']\n",
        "test = monthly_sales[monthly_sales['ds'] >= '2017-01-01']\n",
        "# We enable yearly seasonality as we know there are strong seasonal patterns in the data.\n",
        "prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
        "prophet_model.fit(train)\n",
        "# Create a dataframe for future dates (all 12 months of 2017)\n",
        "future_dates = prophet_model.make_future_dataframe(periods=12, freq='M')\n",
        "# Generate the forecast\n",
        "prophet_forecast = prophet_model.predict(future_dates)\n",
        "# Isolate the predictions for the test period (2017)\n",
        "prophet_pred = prophet_forecast[prophet_forecast['ds'] >= '2017-01-01']['yhat']\n",
        "# Calculate the Mean Absolute Percentage Error (MAPE)\n",
        "mape_prophet = mean_absolute_percentage_error(test['y'], prophet_pred) * 100\n",
        "print(\"Prophet Model Performance (MAPE)\")\n",
        "print(f\"Prophet Model MAPE for 2017 Forecast: {mape_prophet:.2f}%\")\n",
        "print(\"\\nMAPE indicates the average percentage difference between the forecast and the actual sales.\")\n",
        "\n",
        "# Use Prophet's built-in plotting function for a detailed view\n",
        "fig1 = prophet_model.plot(prophet_forecast)\n",
        "fig1.suptitle(\"Prophet Forecast Components\", y=1.02)\n",
        "\n",
        "# Create a cleaner plot for comparison\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(x=train['ds'], y=train['y'], mode='lines', name='Training Data'))\n",
        "fig2.add_trace(go.Scatter(x=test['ds'], y=test['y'], mode='lines', name='Actual Sales (2017)', line=dict(color='black', width=3)))\n",
        "fig2.add_trace(go.Scatter(x=test['ds'], y=prophet_pred, mode='lines', name='Prophet Forecast', line=dict(color='red', width=3, dash='dot')))\n",
        "\n",
        "fig2.update_layout(title='Prophet Forecast vs. Actual Sales for 2017',\n",
        "                   xaxis_title='Date',\n",
        "                   yaxis_title='Sales')\n",
        "fig2.show()\n"
      ],
      "metadata": {
        "id": "XTgi8akXrlGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecast Scenario Planning"
      ],
      "metadata": {
        "id": "NGylZKSot5Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "monthly_sales = df.resample('M', on='Order Date')['Sales'].sum().reset_index()\n",
        "monthly_sales.rename(columns={'Order Date': 'ds', 'Sales': 'y'}, inplace=True)\n",
        "\n",
        "prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
        "prophet_model.fit(monthly_sales) # Fit on all data for a future forecast\n",
        "future_dates = prophet_model.make_future_dataframe(periods=12, freq='M')\n",
        "prophet_forecast = prophet_model.predict(future_dates)\n",
        "\n",
        "# The forecast dataframe already contains the different scenarios.\n",
        "# 'yhat' is the most-likely forecast.\n",
        "# 'yhat_lower' is the pessimistic (worst-case) scenario.\n",
        "# 'yhat_upper' is the optimistic (best-case) scenario.\n",
        "forecast_period = prophet_forecast[prophet_forecast['ds'] > monthly_sales['ds'].max()]\n",
        "fig = go.Figure()\n",
        "# Plot historical data\n",
        "fig.add_trace(go.Scatter(x=monthly_sales['ds'], y=monthly_sales['y'], mode='lines', name='Historical Sales'))\n",
        "# Plot Most-Likely Forecast\n",
        "fig.add_trace(go.Scatter(x=forecast_period['ds'], y=forecast_period['yhat'], mode='lines', name='Most-Likely Forecast', line=dict(color='blue', width=3)))\n",
        "# Plot Best-Case Scenario\n",
        "fig.add_trace(go.Scatter(x=forecast_period['ds'], y=forecast_period['yhat_upper'], mode='lines', name='Best-Case Scenario', line=dict(color='green', dash='dot')))\n",
        "# Plot Worst-Case Scenario\n",
        "fig.add_trace(go.Scatter(x=forecast_period['ds'], y=forecast_period['yhat_lower'], mode='lines', name='Worst-Case Scenario', line=dict(color='red', dash='dot')))\n",
        "fig.update_layout(\n",
        "    title='Sales Forecast Scenario Planning for the Next 12 Months',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Monthly Sales',\n",
        "    legend_title='Scenario'\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tuSbPWW5sp4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Market Intelligence Forecasting"
      ],
      "metadata": {
        "id": "bCH_7F4EVtJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "monthly_sales = df.resample('M', on='Order Date')['Sales'].sum().reset_index()\n",
        "monthly_sales.rename(columns={'Order Date': 'ds', 'Sales': 'y'}, inplace=True)\n",
        "# External Data: US Quarterly GDP Growth Rate (%)\n",
        "# For this project, this data is provided. In a real-world scenario, you would source this from an economic data provider.\n",
        "gdp_data = {\n",
        "    'date': pd.to_datetime(['2014-01-01', '2014-04-01', '2014-07-01', '2014-10-01',\n",
        "                           '2015-01-01', '2015-04-01', '2015-07-01', '2015-10-01',\n",
        "                           '2016-01-01', '2016-04-01', '2016-07-01', '2016-10-01',\n",
        "                           '2017-01-01', '2017-04-01', '2017-07-01', '2017-10-01',\n",
        "                           '2018-01-01', '2018-04-01', '2018-07-01', '2018-10-01']),\n",
        "    'gdp_growth': [-1.2, 4.5, 5.1, 2.1, 3.2, 3.4, 2.0, 1.1, 1.5, 2.0, 2.7, 2.4, 2.0, 2.2, 2.9, 3.5, 2.2, 3.5, 2.9, 1.1]\n",
        "}\n",
        "gdp_df = pd.DataFrame(gdp_data)\n",
        "gdp_df['ds'] = gdp_df['date']\n",
        "# using forward-fill merge algorithm  to apply the quarterly GDP data to each month within that quarter.\n",
        "merged_df = pd.merge_asof(monthly_sales.sort_values('ds'),gdp_df.sort_values('ds'),on='ds',direction='backward')\n",
        "# Build and Train the Multivariate Prophet Model\n",
        "# Initialize the model\n",
        "model_with_gdp = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
        "# Add the external regressor\n",
        "model_with_gdp.add_regressor('gdp_growth')\n",
        "# Fit the model on the combined data\n",
        "model_with_gdp.fit(merged_df)\n",
        "# To forecast we need future values for our regressor. We'll assume a stable growth rate for the next year\n",
        "future = model_with_gdp.make_future_dataframe(periods=12, freq='M')\n",
        "#adding the gdp_growth column to our future dataframe --  this is for future analysis too\n",
        "future_gdp = pd.merge_asof(future.sort_values('ds'),gdp_df.sort_values('ds'),on='ds',direction='backward')\n",
        "# For dates beyond our known GDP data, we'll assume a stable 2.5% growth\n",
        "future_gdp['gdp_growth'].fillna(2.5, inplace=True)\n",
        "forecast_with_gdp = model_with_gdp.predict(future_gdp)\n",
        "fig = model_with_gdp.plot(forecast_with_gdp)\n",
        "fig.suptitle(\"Sales Forecast with GDP Growth as an External Factor\", y=1.02)\n",
        "# Plot the model components, including the new GDP component\n",
        "fig_components = model_with_gdp.plot_components(forecast_with_gdp)\n"
      ],
      "metadata": {
        "id": "H6iHyX8SuA_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strategic Recommendations Engine"
      ],
      "metadata": {
        "id": "ckkk3nL4WUlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(df):\n",
        "    \"\"\"\n",
        "    Analyzes the superstore dataframe to generate a prioritized list of strategic recommendations.\n",
        "    \"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    # opportunity : Address Unprofitable Sub-Categories\n",
        "    subcat_profit = df.groupby('Sub-Category')['Profit'].sum()\n",
        "    unprofitable_subcats = subcat_profit[subcat_profit < 0]\n",
        "    for subcat, loss in unprofitable_subcats.items():\n",
        "        recommendations.append({\n",
        "            'Opportunity': f\"Address Unprofitable Sub-Category: {subcat}\",\n",
        "            'Impact': abs(loss),  # Impact is the total loss that can be saved\n",
        "            'Effort': 6,  # Medium-High effort: requires investigation, pricing changes, etc.\n",
        "            'Recommendation': f\"The '{subcat}' sub-category lost ${abs(loss):,.2f}. Investigate cost structure, pricing, and discount strategy. Consider discontinuing if profitability cannot be improved.\"\n",
        "        })\n",
        "\n",
        "    #  Opportunity : Expand in Untapped Markets\n",
        "    customer_per_state = df.groupby('State')['Customer ID'].nunique()\n",
        "    untapped_states = customer_per_state[customer_per_state < 10]\n",
        "    avg_sales_per_customer = df['Sales'].sum() / df['Customer ID'].nunique()\n",
        "    for state, num_customers in untapped_states.items():\n",
        "        potential_impact = avg_sales_per_customer * 50 # Estimate impact of acquiring 50 new customers\n",
        "        recommendations.append({\n",
        "            'Opportunity': f\"Expand in Untapped Market: {state}\",\n",
        "            'Impact': potential_impact,\n",
        "            'Effort': 8,  # High effort: requires marketing campaigns, logistics\n",
        "            'Recommendation': f\"The state of '{state}' has only {num_customers} customers. Launch a targeted digital marketing campaign to increase market penetration and acquire new customers.\"\n",
        "        })\n",
        "\n",
        "    # Opportunity 3: Re-engage High-Value, At-Risk Customers\n",
        "    # (Using a simplified RFM logic for this engine)\n",
        "    snapshot_date = df['Order Date'].max() + pd.Timedelta(days=1)\n",
        "    rfm = df.groupby('Customer ID').agg({\n",
        "        'Order Date': lambda date: (snapshot_date - date.max()).days,\n",
        "        'Sales': 'sum'\n",
        "    })\n",
        "    rfm.rename(columns={'Order Date': 'Recency', 'Sales': 'MonetaryValue'}, inplace=True)\n",
        "\n",
        "    # Defining  'At-Risk': High value (top 25%) but not recent (recency > 180 days)\n",
        "    at_risk_customers = rfm[(rfm['MonetaryValue'] > rfm['MonetaryValue'].quantile(0.75)) & (rfm['Recency'] > 180)]\n",
        "    if not at_risk_customers.empty:\n",
        "        at_risk_impact = at_risk_customers['MonetaryValue'].sum()\n",
        "        recommendations.append({\n",
        "            'Opportunity': \"Re-engage High-Value, At-Risk Customers\",\n",
        "            'Impact': at_risk_impact,\n",
        "            'Effort': 3,\n",
        "            'Recommendation': f\"There are {len(at_risk_customers)} high-value customers who have not purchased in over 6 months. Recommendation is to Launch a targeted re-engagement campaign with a special offer to win them back.\"\n",
        "        })#giving suggestion for regaining the lost customers\n",
        "    #  Scoring and Ranking\n",
        "    rec_df = pd.DataFrame(recommendations)\n",
        "    rec_df['Impact_Score'] = rec_df['Impact'].rank(pct=True)\n",
        "    rec_df['Effort_Score'] = rec_df['Effort'].rank(pct=True)\n",
        "    rec_df['Priority_Score'] = rec_df['Impact_Score'] / rec_df['Effort_Score']\n",
        "    # Sort by the highest priority\n",
        "    rec_df.sort_values('Priority_Score', ascending=False, inplace=True)\n",
        "    return rec_df\n",
        "\n",
        "top_recommendations = generate_recommendations(df)\n",
        "for index, row in top_recommendations.head(5).iterrows():\n",
        "    print(f\"\\nOPPORTUNITY: {row['Opportunity']}\")\n",
        "    print(f\"  - PRIORITY SCORE: {row['Priority_Score']:.2f}\")\n",
        "    print(f\"  - RECOMMENDATION: {row['Recommendation']}\")\n"
      ],
      "metadata": {
        "id": "07Q3ADNzWJeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lrB1l2Lb7c9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVbRMev3lyXTqNKTUo41MC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}